---
title: "Modeling with tidymodels in R - Chapter 02"
author: "Edneide Ramalho"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output:
  html_document:
    highlight: textmate
    logo: logo.png
    theme: jou
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
    df_print: paged
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri("img/tidymodels.png"), 
               alt = 'logo', 
               style = 'position:absolute; top:0; right:0; padding:10px;',
               width = "200",
               heigth = "200")
```

# Imports

```{r packages}
library(tidyverse)
library(tidymodels)
```

# Classification models

```{r}
# data
leads_df <- readRDS('datasets/leads_df.rds')
glimpse(leads_df)
```

## Data resampling

```{r}
leads_split <- initial_split(leads_df,
                             prop = 0.75,
                             strata = purchased)

leads_training <- leads_split %>% 
  training()

leads_test <- leads_split %>%
  testing()

```

## Logistic regression model specification

```{r}
logistic_model <- logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')
```

## Model fitting

```{r}
logistic_fit <- logistic_model %>% 
  fit(purchased ~ total_visits + total_time,
      data = leads_training)
```

## Predicting outcome categories

```{r}
class_preds <- logistic_fit %>% 
  predict(new_data = leads_test, 
          type = 'class')

class_preds
```

## Estimated probabilities 

```{r}
prob_preds <- logistic_fit %>% 
  predict(new_data = leads_test,
          type = 'prob')

prob_preds
```

## Combining results

```{r}
leads_results <- leads_test %>% 
  select(purchased) %>% 
  bind_cols(class_preds, prob_preds)

leads_results
```

# Exercises - part 1

> 1.  **Data resampling**

The first step in a machine learning project is to create training and test datasets for model fitting and evaluation. The test dataset provides an estimate of how your model will perform on new data and helps to guard against overfitting.

You will be working with the `telecom_df` dataset which contains information on customers of a telecommunications company. The outcome variable is `canceled_service` and it records whether a customer canceled their contract with the company. The predictor variables contain information about customers' cell phone and Internet usage as well as their contract type and monthly charges.

The `telecom_df` tibble has been loaded into your session.

-   Create an `rsample` object, `telecom_split`, that contains the instructions for randomly splitting the `telecom_df` data into training and test datasets.

    -   Allocate 75% of the data into training and stratify the results by `canceled_service`.

-   Pass the `telecom_split` object to the appropriate `rsample` functions to create the training and test datasets.

-   Check the number of rows in each datasets by passing them to the `nrow()` function.

```{r}
# data
telecom_df <- readRDS('datasets/telecom_df.rds')
glimpse(telecom_df)

# Create data split object
telecom_split <- initial_split(telecom_df, prop = 0.75,
                     strata = canceled_service)

# Create the training data
telecom_training <- telecom_split %>% 
  training()

# Create the test data
telecom_test <- telecom_split %>% 
  testing()

# Check the number of rows
paste0("#Rows training: ", nrow(telecom_training))
paste0("#Rows test: ", nrow(telecom_test))
```

# Fitting a logistic regression model

In addition to regression models, the `parsnip` package also provides a general interface to classification models in R.

In this exercise, you will define a `parsnip` logistic regression object and train your model to predict `canceled_service` using `avg_call_mins`, `avg_intl_mins`, and `monthly_charges` as predictor variables from the `telecom_df` data.

-   Initialize a logistic regression object, `logistic_model`, with the appropriate `parsnip` function.

-   Use the `'glm'` engine.

-   Set the mode to `'classification'`.

-   Print the `logistic_model` object to view its specification details.

-   Train your model to predict `canceled_service` using `avg_call_mins`, `avg_intl_mins`, and `monthly_charges` as predictor variables from the `telecom_training` dataset.

```{r}
# Specify a logistic regression model
logistic_model <- logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')

# Print the model specification
logistic_model
```

```{r}
# Fit to training data
logistic_fit <- logistic_model %>% 
  fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges, data = telecom_training)

# Print model fit object
logistic_fit
```

> 2.  **Combining test dataset results**

Evaluating your model's performance on the test dataset gives insights into how well your model predicts on new data sources. These insights will help you communicate your model's value in solving problems or improving decision making.

Before you can calculate classification metrics such as sensitivity or specificity, you must create a results tibble with the required columns for `yardstick` metric functions.

In this exercise, you will use your trained model to predict the outcome variable in the `telecom_test` dataset and combine it with the true outcome values in the `canceled_service` column.

-   Use your trained model and the `predict()` function to create a tibble, `class_preds`, with predicted outcome variable categories using the test dataset.

```{r}
# Predict outcome categories
class_preds <- predict(logistic_fit, new_data = telecom_test,
                       type = 'class') 
class_preds
```

-   Now create a tibble, `prob_preds`, with the estimated probabilities for each category in the outcome variable using the test dataset.

```{r}
# Obtain estimated probabilities for each outcome value
prob_preds <- predict(logistic_fit, new_data = telecom_test,
                       type = 'prob') 
prob_preds
```

-   Select the outcome variable from the `telecom_test` data.

-   Add the `class_preds` and `prob_preds` tibbles along the column axis.

```{r}
telecom_results <- telecom_test %>% 
  select(canceled_service) %>% 
  bind_cols(class_preds, prob_preds)
```

# Assessing model fit 

In `tidymodels` outcome variables needs to be a factor

-   First level is positive class

-   Check order with `levels()`

```{r}
leads_df %>% glimpse()

levels(leads_df$purchased)
```

## Confusion matrix

```{r, echo=FALSE}
# htmltools::img(src = knitr::image_uri("img/confusion_matrix.png"),
#                width = "200",
#                heigth = "200",
#                align = 'center')
```

```{r, echo=FALSE}
library(htmltools)
# Replace "your_image.png" with the actual filename of your image
img_path <- "img/confusion_matrix.png"

# Create the image container with CSS styles to center it
img_centered <- div(
  style = "display: flex; justify-content: center;",
  img(src = img_path, width = "350")
)
img_centered
```

**Correct predictions**

-   True Positive (TP)

-   True Negative (TN)

**Classifcation Errors**

-   False Positive (FP)

-   False Negative (FN)

```{r}
conf_mat(leads_results,
         truth = purchased,
         estimate = .pred_class)
```

## Classification accuracy

$$
\dfrac{TP + TN}{TP + TN + FP + FN}
$$

```{r}
accuracy(leads_results, 
         truth = purchased,
         estimate = .pred_class)
```

## Sensitivity

In many cases accuracy is not the best metric.

-   `leads_df` data

    -   Classifying all as 'no' gives 64% accuracy

**Sensitivity**

Proportion of all positive cases that were correctly classified.

-   Of custumers who did purchased, what proportion did our model predicted correctly?

    -   Lower false negatives increase sensitivity.

$$
\dfrac{TP}{TP + FN}
$$

```{r}
sens(leads_results, 
         truth = purchased,
         estimate = .pred_class)
```

## Specificity

$$
\dfrac{TN}{TN + FP}
$$

Is the proportion of all negative cases that were correctly classified.

-   Of customers who did not purchased, what proportion did our model predicted correctly?

    -    Lower false positive increases specificity

$1 - \text{specificity}$ : False Positive Rate (FPR)

-   Proportion of false positives among True negatives

```{r}
spec(leads_results, 
         truth = purchased,
         estimate = .pred_class)
```

## Creating a metric set

```{r}
custom_metrics <- metric_set(accuracy, sens, spec)

custom_metrics(leads_results,
               truth = purchased,
               estimate = .pred_class)
```

## Many metrics

-   `accuracy()`, `kap()`, `sens()`, `spec()`, `ppv()`, `npv(),` `mcc()`, `j_index()`, `bal_accuracy()`, `detection_prevalence()`, `precision()`, `recall()`, `f_means()`

-   Pass the results of conf_mat() to summary() to calculate all

-   Documentation: <https://yardstick.tidymodels.org/reference>

# Exercises - part 1

> 1.  **Evaluating performance with yardstick**

In the previous exercise, you calculated classification metrics from a sample confusion matrix. The `yardstick` package was designed to automate this process.

For classification models, `yardstick` functions require a tibble of model results as the first argument. This should include the actual outcome values, predicted outcome values, and estimated probabilities for each value of the outcome variable.

In this exercise, you will use the results from your logistic regression model, `telecom_results`, to calculate performance metrics.

-   Use the appropriate `yardstick` function to create a confusion matrix using the `telecom_results` tibble.

    ```{r}
    # Calculate the confusion matrix
    conf_mat(telecom_results, truth = canceled_service,
        estimate = .pred_class)
    ```

-   Calculate the **accuracy** of your model with the appropriate `yardstick` function.

    ```{r}
    # Calculate the accuracy
    accuracy(telecom_results, truth = canceled_service,
        estimate = .pred_class)
    ```

-   Calculate the **sensitivity** of your model.

    ```{r}
    # Calculate the sensitivity
    sens(telecom_results, truth = canceled_service,
        estimate = .pred_class)
    ```

-   Calculate the **specificity** of your model.

    ```{r}
    # Calculate the specificity
    spec(telecom_results,
                truth = canceled_service, 
                estimate = .pred_class)
    ```

> **2. Creating custom metric sets**

The `yardstick` package also provides the ability to create custom sets of model metrics. In cases where the cost of obtaining false negative errors is different from the cost of false positive errors, it may be important to examine a specific set of performance metrics.

Instead of calculating accuracy, sensitivity, and specificity separately, you can create your own metric function that calculates all three at the same time.

In this exercise, you will use the results from your logistic regression model, `telecom_results`, to calculate a custom set of performance metrics. You will also use a confusion matrix to calculate all available binary classification metrics in `tidymodels`all at once.

```{r}
# Create a custom metric function
telecom_metrics <- metric_set(accuracy, sens, spec)

# Calculate metrics using model results tibble
telecom_metrics(telecom_results, truth = canceled_service,
                estimate = .pred_class)

# Create a confusion matrix
conf_mat(telecom_results,
         truth = canceled_service,
         estimate = .pred_class) %>% 
  # Pass to the summary() function
  summary()
```

# Visualizing the confusion matrix

## Plotting confusion matrix

```{r}
conf_mat(leads_results,
         truth = purchased,
         estimate = .pred_class) %>% 
  autoplot(type = 'heatmap')
```

## Mosaic plot

```{r}
conf_mat(leads_results,
         truth = purchased,
         estimate = .pred_class) %>% 
  autoplot(type = 'mosaic')
```

## Probability thresholds

-   Default probability in binary classification is 0.5

## Exploring performance across thresholds

How does a classification model perform across a range of thresholds?

-   Unique probabulity thresholds in the `.pred_yes` column of the dataset results

-   Calculate specificity and sensitivity for each

## Visualizing performance across thresholds

-   ROC (Receiver operating characteristics) curve

-   Used to visualize performance across probability thresholds

-   Sensitivity vs (1 - specificity) across unique thresholds in test set results

    -   Proportion correct among actual positives vs. proportion incorrect among actual negatives

-   ROC AUC: Area Under the Curve

-   Interpretation as letter for grades

    -   A - [0.9, 1]

    -   B - [0.8, 0.9)

    -   C - [0.7, 0.8)

    -   D - [0.6, 0.7)

    -   F - [0.5 0.6)

```{r}
leads_results %>% 
  roc_curve(truth = purchased, .pred_yes) %>% 
  autoplot()
```

```{r}
# Calculating AUC
roc_auc(leads_results,
        truth = purchased,
        .pred_yes)
```

# Exercises

> 1.  **Plotting the confusion matrix**

Calculating performance metrics with the `yardstick` package provides insight into how well a classification model is performing on the test dataset. Most `yardstick` functions return a single number that summarizes classification performance.

Many times, it is helpful to create visualizations of the confusion matrix to more easily communicate your results.

In this exercise, you will make a heat map and mosaic plot of the confusion matrix from your logistic regression model on the `telecom_df` dataset.

Your model results tibble, `telecom_results`, has been loaded into your session.

-   Create a confusion matrix from your model results, `telecom_results`.

-   Pass your confusion matrix to the appropriate function for creating heat maps and mosaic plots.

```{r}
# Create a confusion matrix
conf_mat(telecom_results,
         truth = canceled_service,
         estimate = .pred_class) %>% 
  # Create a heat map
  autoplot(type = 'heatmap')
```

```{r}
# Create a confusion matrix
conf_mat(telecom_results,
         truth = canceled_service,
         estimate = .pred_class) %>% 
  # Create a heat map
  autoplot(type = 'mosaic')
```

> 2.  **ROC curves and area under the ROC curve**

ROC curves are used to visualize the performance of a classification model across a range of probability thresholds. An ROC curve with the majority of points near the upper left corner of the plot indicates that a classification model is able to correctly predict both the positive and negative outcomes correctly across a wide range of probability thresholds.

The area under this curve provides a letter grade summary of model performance.

In this exercise, you will create an ROC curve from your logistic regression model results and calculate the area under the ROC curve with `yardstick`.

Your model results tibble, `telecom_results` has been loaded into your session.

-   Create a tibble, `threshold_df`, which contains the sensitivity and specificity of your classification model across the unique probability thresholds in `telecom_results`.

-   Print `threshold_df` to view the results.

-   Use `threshold_df` to to plot your model's ROC curve.

-   Calculate the area under the ROC curve using the `telecom_results` tibble.

```{r}
# Calculate metrics across thresholds
threshold_df <- telecom_results %>% 
  roc_curve(truth = canceled_service, .pred_yes)

# View results
threshold_df

# Plot ROC curve
threshold_df %>% 
  autoplot()

# Calculate ROC AUC
roc_auc(telecom_results,
    truth = canceled_service, 
    .pred_yes)
```

# **Automating the modeling workflow**

```{r}
leads_split <- initial_split(leads_df,
                             strata = purchased)

logistic_model <- logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')
```

```{r}
logistic_last_fit <- logistic_model %>% 
  last_fit(purchased ~ total_visits + total_time,
           split = leads_split)

logistic_last_fit %>% 
  collect_metrics()
```

## Collecting predictions

```{r}
last_fit_results <- logistic_last_fit %>% 
  collect_predictions()

last_fit_results
```

## Custom metric sets 

-   accuracy(), sens(), and spec()

    -   Requires truth and estimate

-   roc_auc()

    -   Requires truth and column of estimated probabilities

```{r}
custom_metrics <- metric_set(accuracy, sens, spec, roc_auc)

custom_metrics(last_fit_results,
               truth = purchased,
               estimate = .pred_class,
               .pred_yes)
```

# Exercises

> 1.  **Streamlining the modeling process**

The `last_fit()` function is designed to streamline the modeling workflow in `tidymodels`. Instead of training your model on the training data and building a results tibble using the test data, `last_fit()` accomplishes this with one function.

In this exercise, you will train the same logistic regression model as you fit in the previous exercises, except with the `last_fit()` function.

Your data split object, `telecom_split`, and model specification, `logistic_model`, have been loaded into your session.

-   Pass your `logistic_model` object into the `last_fit()` function.

-   Predict `canceled_service` using `avg_call_mins`, `avg_intl_mins`, and `monthly_charges`.

-   Display the performance metrics of your trained model, `telecom_last_fit`.

```{r}
# Train model with last_fit()
telecom_last_fit <- logistic_model %>% 
  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges,
           split = telecom_split)

# View test set metrics
telecom_last_fit %>% 
  collect_metrics()
```

> 2.  **Collecting predictions and creating custom metrics**

Using the `last_fit()` modeling workflow also saves time in collecting model predictions. Instead of manually creating a tibble of model results, there are helper functions that extract this information automatically.

In this exercise, you will use your trained model, `telecom_last_fit`, to create a tibble of model results on the test dataset as well as calculate custom performance metrics.

Your trained model, `telecom_last_fit`, has been loaded into this session.

-   Create a tibble, `last_fit_results`, that has the predictions from your `telecom_last_fit` model.

-   Print the results to the console.

-   Create a custom metric function, `last_fit_metrics`, using the `metric_set()` function.

-   Include the accuracy, sensitivity, specificity, and area under the ROC curve in your metric function, in that order.

-   Use the `last_fit_metrics()` function to calculate your custom metrics on the `last_fit_results` tibble.

```{r}
# Collect predictions
last_fit_results <- telecom_last_fit %>% 
  collect_predictions()

# View results
last_fit_results

# Custom metrics function
last_fit_metrics <- metric_set(accuracy, sens,
                               spec, roc_auc)

# Calculate metrics
last_fit_metrics(last_fit_results,
                 truth = canceled_service,
                 estimate = .pred_class,
                 .pred_yes)
```

> 3.  **Complete modeling workflow**

In this exercise, you will use the `last_fit()` function to train a logistic regression model and evaluate its performance on the test data by assessing the ROC curve and the area under the ROC curve.

Similar to previous exercises, you will predict `canceled_service` in the `telecom_df` data, but with an additional predictor variable to see if you can improve model performance.

The `telecom_df` tibble, `telecom_split`, and `logistic_model` objects from the previous exercises have been loaded into your workspace. The `telecom_split` object contains the instructions for randomly splitting the `telecom_df` tibble into training and test sets. The `logistic_model` object is a `parsnip` specification of a logistic regression model.

-   Train your model to predict `canceled_service` using `avg_call_mins`, `avg_intl_mins`, `monthly_charges`, and `months_with_company`.

-   Collect and print the performance metrics on the test dataset.

-   Collect your model predictions.

-   Pass the predictions to the appropriate function to calculate sensitivity and specificity for different probability thresholds.

-   Pass the results to the appropriate plotting function to create an ROC curve.

```{r}
# Train a logistic regression model
logistic_fit <- logistic_model %>% 
  last_fit(canceled_service ~ avg_call_mins + avg_intl_mins + monthly_charges + months_with_company, 
           split = telecom_split)

# Collect metrics
logistic_fit %>% 
  collect_metrics()

# Collect model predictions
logistic_fit %>% 
  collect_predictions() %>% 
  # Plot ROC curve
  roc_curve(truth = canceled_service, .pred_yes) %>% 
  autoplot()
```
